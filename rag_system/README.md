# SystÃ¨me RAG avec ElasticSearch et LangChain

Ce projet implÃ©mente un systÃ¨me de Retrieval Augmented Generation (RAG) utilisant ElasticSearch comme base de donnÃ©es vectorielle, LangChain pour le traitement et l'orchestration, et Streamlit pour l'interface utilisateur.

## FonctionnalitÃ©s

- ğŸ“„ Pipeline pour collecter, traiter et indexer des documents (JSON/PDF/TXT)
- ğŸ” Recherche sÃ©mantique avec ElasticSearch et embeddings
- ğŸ¤– Service de question-rÃ©ponse utilisant Gemini API ou Ollama local
- ğŸ–¥ï¸ Interface utilisateur Streamlit pour tÃ©lÃ©charger des documents et poser des questions
- ğŸ³ DÃ©ploiement Docker avec support GPU possible

## PrÃ©requis

- Python 3.10+
- Docker et Docker Compose
- ClÃ© API Gemini (si vous utilisez le modÃ¨le Gemini)
- Ollama installÃ© localement (si vous utilisez Ollama)

## Installation

1. Clonez ce dÃ©pÃ´t :
   ```bash
   git clone <URL_DU_REPO>
   cd rag_system
   ```

2. Installez les dÃ©pendances Python :
   ```bash
   # Sur Linux/macOS
   make setup
   
   # OU manuellement
   pip install -r requirements.txt
   cp .env.example .env
   ```

3. Configurez vos variables d'environnement dans le fichier `.env` :
   - DÃ©finissez `GEMINI_API_KEY` si vous utilisez Gemini
   - Ou configurez `OLLAMA_URL` si vous utilisez Ollama

## Test du systÃ¨me

Vous pouvez tester le systÃ¨me avant de le dÃ©ployer complÃ¨tement :

```bash
# Sur Linux/macOS
make test

# Sur Windows ou manuellement
python test_system.py
```

Ce test vÃ©rifie la connexion Ã  ElasticSearch, le traitement des documents et la gÃ©nÃ©ration de rÃ©ponses.

## DÃ©ploiement

### Avec Docker (recommandÃ©)

Vous pouvez dÃ©ployer l'ensemble du systÃ¨me avec Docker Compose :

```bash
# Sur Linux/macOS avec Make
make deploy-cpu   # Version CPU
make deploy-gpu   # Version avec support GPU

# Sur Windows
deploy.bat        # Version CPU
deploy.bat -g     # Version avec support GPU

# OU manuellement
docker-compose up -d                  # Version CPU
docker-compose -f docker-compose.gpu.yml up -d  # Version avec support GPU
```

L'application sera accessible Ã  l'adresse : http://localhost:8501

### Sans Docker

Pour exÃ©cuter l'application sans Docker, vous devez d'abord dÃ©marrer ElasticSearch sÃ©parÃ©ment :

1. TÃ©lÃ©chargez et installez ElasticSearch 8.10.0
2. DÃ©marrez ElasticSearch
3. Lancez l'application Streamlit :

```bash
# Sur Linux/macOS
make run

# Sur Windows ou manuellement
python main.py run
```

## Utilisation

### Interface utilisateur Streamlit

AccÃ©dez Ã  http://localhost:8501 pour utiliser l'interface Streamlit.

- **TÃ©lÃ©chargement de documents** : Utilisez la colonne de droite pour tÃ©lÃ©charger des fichiers PDF, TXT ou JSON.
- **Poser des questions** : Entrez votre question dans la zone de chat et recevez une rÃ©ponse gÃ©nÃ©rÃ©e Ã  partir des documents pertinents.
- **Voir les sources** : Les sources utilisÃ©es pour gÃ©nÃ©rer la rÃ©ponse sont affichÃ©es en dessous de celle-ci.

### Commandes en ligne

Vous pouvez Ã©galement utiliser des commandes pour interagir avec le systÃ¨me :

```bash
# Indexer des documents
python main.py index --directory /chemin/vers/vos/documents

# Effacer l'index
python main.py clear
```

## ArrÃªt et nettoyage

```bash
# Sur Linux/macOS avec Make
make stop-containers  # ArrÃªter les conteneurs
make clean-volumes    # Nettoyer les volumes

# Sur Windows
deploy.bat -v         # Voir l'Ã©tat des conteneurs
docker-compose down   # ArrÃªter les conteneurs
```

## Structure du projet

```
rag_system/
â”‚
â”œâ”€â”€ app/                  # Interface utilisateur
â”‚   â””â”€â”€ streamlit_app.py  # Application Streamlit
â”‚
â”œâ”€â”€ config/               # Configuration
â”‚   â””â”€â”€ config.py         # Configuration du systÃ¨me
â”‚
â”œâ”€â”€ core/                 # Logique principale
â”‚   â”œâ”€â”€ elasticsearch_manager.py  # Gestion d'ElasticSearch
â”‚   â”œâ”€â”€ indexing_pipeline.py      # Pipeline d'indexation
â”‚   â”œâ”€â”€ llm_service.py            # Service LLM
â”‚   â””â”€â”€ rag_service.py            # Service RAG principal
â”‚
â”œâ”€â”€ data/                 # DonnÃ©es
â”‚   â”œâ”€â”€ documents/        # Documents Ã  indexer
â”‚   â””â”€â”€ embeddings/       # Embeddings gÃ©nÃ©rÃ©s
â”‚
â”œâ”€â”€ utils/                # Utilitaires
â”‚   â””â”€â”€ document_processor.py  # Traitement des documents
â”‚
â”œâ”€â”€ .env.example          # Exemple de fichier .env
â”œâ”€â”€ deploy.bat            # Script de dÃ©ploiement Windows
â”œâ”€â”€ deploy.sh             # Script de dÃ©ploiement Unix
â”œâ”€â”€ docker-compose.yml    # Configuration Docker Compose (CPU)
â”œâ”€â”€ docker-compose.gpu.yml # Configuration Docker Compose (GPU)
â”œâ”€â”€ Dockerfile            # Configuration de l'image Docker
â”œâ”€â”€ Makefile              # Commandes Make
â”œâ”€â”€ main.py               # Point d'entrÃ©e de l'application
â”œâ”€â”€ test_system.py        # Tests du systÃ¨me
â”œâ”€â”€ README.md             # Documentation
â””â”€â”€ requirements.txt      # DÃ©pendances Python
```

## Personnalisation

- **ModÃ¨le d'embedding** : Modifiez la variable `EMBEDDING_MODEL` dans le fichier `.env` pour utiliser un modÃ¨le d'embedding diffÃ©rent.
- **Fournisseur LLM** : Choisissez entre `gemini` et `ollama` en modifiant la variable `LLM_PROVIDER`.
- **Configuration ElasticSearch** : Modifiez les paramÃ¨tres d'ElasticSearch dans le fichier `docker-compose.yml`.

## RÃ©solution des problÃ¨mes

- **Erreur de connexion Ã  ElasticSearch** : VÃ©rifiez que ElasticSearch est bien dÃ©marrÃ© et accessible Ã  l'adresse http://localhost:9200.
- **ProblÃ¨me avec les embeddings** : Assurez-vous que le modÃ¨le d'embedding spÃ©cifiÃ© est disponible et que vous avez une connexion Internet active pour le tÃ©lÃ©charger.
- **Erreur avec Gemini API** : VÃ©rifiez que votre clÃ© API Gemini est correctement configurÃ©e dans le fichier `.env`.
- **ProblÃ¨me avec Ollama** : Assurez-vous qu'Ollama est en cours d'exÃ©cution et accessible Ã  l'URL spÃ©cifiÃ©e.

## Licence

Ce projet est sous licence MIT. 